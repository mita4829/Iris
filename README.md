# Iris

![Alt text](https://github.com/mita4829/Iris/blob/master/Header.jpg "Iris a1.0")
<b>Iris b1.0 Xcode 8, 8.1, 8.2, 8.3</b><br/>
[![Build Status](https://travis-ci.org/mita4829/Iris.svg?branch=master)](https://travis-ci.org/mita4829/Iris)

## Description
The goal of this project is to design an iOS app to allow people who are visually-impaired to easily read text found on images. Users can take a picture of the text with their iPhone, and Iris will convert the text to pseudo-braille. The users can interpret the text with vibrations on their iPhone.

![Alt text](https://github.com/mita4829/Iris/blob/master/iris.jpg)

## Tools Used
* [GitHub](https://github.com/) - Version Control
* [Travis CI](https://travis-ci.org/) - Continuous Integration tool
* [Slack](https://slack.com/) - Collaboration
* [Xcode](https://developer.apple.com/xcode/) - IDE
* [Swift 3](https://swift.org) - Programming Language
* [Objective-C](https://developer.apple.com/reference/objectivec) - Programming Language
* [Tesseract](https://github.com/tesseract-ocr/tesseract) - OCR Library

## Authors
* **Ross Blassingame** - *App Design* - [GitHub](https://github.com/RossBlassingame)
* **Ryan Baten** - *Research* - [GitHub](https://github.com/RyanBaten)
* **Michael Tang** - *Implemented Tesseract* - [GitHub](https://github.com/mita4829)

See also the list of [contributors](https://github.com/mita4829/Iris/graphs/contributors) who participated in this project.
